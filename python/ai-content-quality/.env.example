# AI Content Quality Agent - Environment Variables
# Copy this file to .env and fill in your values

# =============================================================================
# LLM CONFIGURATION
# =============================================================================

# Provider: "openai" | "google" | "anthropic"
LLM_PROVIDER=openai
LLM_MODEL=gpt-4.1-nano
LLM_TEMPERATURE=0.3
LLM_TIMEOUT=30.0

# Fallback provider (used if primary provider fails after all retries)
# Set FALLBACK_PROVIDER to a different provider than LLM_PROVIDER
FALLBACK_PROVIDER=google
FALLBACK_MODEL=gemini-2.0-flash

# Provider API keys (only the active provider's key is required)
OPENAI_API_KEY=your-openai-api-key

# Google Gemini:
# LLM_PROVIDER=google
# LLM_MODEL=gemini-3.0-flash-preview
# GOOGLE_API_KEY=your-google-api-key

# Anthropic:
# LLM_PROVIDER=anthropic
# LLM_MODEL=claude-haiku-3-5-20241022
# ANTHROPIC_API_KEY=your-anthropic-api-key

# Ollama (local, no API key required):
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2
#
# Option A — use Ollama already running on your machine:
#   OLLAMA_BASE_URL=http://localhost:11434   (default when running the app locally)
#
# Option B — spin up Ollama inside Docker Compose:
#   docker compose --profile ollama up
#   OLLAMA_BASE_URL=http://ollama:11434      (set this when using the Docker profile)
OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# OPENTELEMETRY / BASE14 SCOUT
# =============================================================================

# Service identification
SERVICE_NAME=ai-content-quality
SCOUT_ENVIRONMENT=development

# Local OTel Collector endpoint (for docker compose setup)
OTLP_ENDPOINT=http://localhost:4318

# Enable/disable telemetry (set to "true" to disable)
OTEL_SDK_DISABLED=false

# Capture prompt/completion content on spans (opt-in, disabled by default for PII safety)
OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=false

# Base14 Scout credentials (for OTel Collector → Scout export)
# Get these from https://app.base14.io/settings/api-keys
SCOUT_CLIENT_ID=your-scout-client-id
SCOUT_CLIENT_SECRET=your-scout-client-secret
SCOUT_TOKEN_URL=https://auth.base14.io/oauth/token
SCOUT_ENDPOINT=https://collector.base14.io

# =============================================================================
# APPLICATION
# =============================================================================

# Prompt version configuration
REVIEW_PROMPT_VERSION=v1
IMPROVE_PROMPT_VERSION=v1
SCORE_PROMPT_VERSION=v1

# Request timeout (seconds) for LLM analysis endpoints
REQUEST_TIMEOUT=60.0

HOST=0.0.0.0
PORT=8000
